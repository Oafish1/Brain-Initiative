{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faad8ba4-ce16-4dc0-9aa5-36095ae3f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allensdk.core.cell_types_cache import CellTypesCache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacc1ad1-f413-4a7f-b9cf-64eb5321cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64121304-2341-4085-bd5a-beacfd9d87fe",
   "metadata": {},
   "source": [
    "# Produce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b067c3f-33a7-4de0-b78d-0f359e4ca046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "ctc = CellTypesCache(manifest_file='cell_types/manifest.json')\n",
    "ephys_features = ctc.get_ephys_features()\n",
    "cells = ctc.get_cells()\n",
    "cells = {c['id']:c for c in cells}\n",
    "\n",
    "# Get meta\n",
    "ef_df = pd.DataFrame(ephys_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5549b39d-d36f-44d9-82a1-859f17771973",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Format data\n",
    "exp, meta = [], []\n",
    "samples = 1024\n",
    "sweep_num = 10\n",
    "for sid in np.random.choice(np.array(ef_df['specimen_id']), samples, replace=False):\n",
    "    data = ctc.get_ephys_data(sid)\n",
    "    try:\n",
    "        assert sweep_num in data.get_experiment_sweep_numbers()\n",
    "    except:\n",
    "        continue\n",
    "    if cells[sid]['dendrite_type'] not in ['spiny', 'aspiny']:\n",
    "        continue\n",
    "    sweep = data.get_sweep(sweep_num)\n",
    "    index_range = sweep[\"index_range\"]\n",
    "    sampling_rate = sweep[\"sampling_rate\"]\n",
    "    i = sweep[\"stimulus\"][index_range[0]:index_range[1]+1] # in A\n",
    "    v = sweep[\"response\"][index_range[0]:index_range[1]+1] # in V\n",
    "    i *= 1e12 # to pA\n",
    "    v *= 1e3 # to mV\n",
    "    t = np.arange(0, len(v)) * (1.0 / sampling_rate)\n",
    "    exp.append({ 'time':t, 'stim':i, 'resp':v })\n",
    "    meta.append([ cells[sid]['dendrite_type'] ])\n",
    "meta = np.array(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1acb00-9a96-41f8-ba6d-b0ee3cf9f4ae",
   "metadata": {},
   "source": [
    "# Making the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0884b13-a3ad-467b-807c-cf48ea41a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Make model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, h_in, h_out=512, num_layers=1, out_dim=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h_in = h_in\n",
    "        self.h_out = h_out\n",
    "        self.num_layers = num_layers\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.rnn = nn.LSTM(h_in, h_out, num_layers)\n",
    "        self.fc = nn.Linear(h_out, out_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X, return_out=False):\n",
    "        out, (hn, cn) = self.rnn(X)\n",
    "        Y = self.fc(hn)\n",
    "        Y = self.activation(Y)\n",
    "        if return_out:\n",
    "            return Y, out\n",
    "        return Y\n",
    "    \n",
    "    def run_all(self, X):\n",
    "        logits = []\n",
    "        for x in X:\n",
    "            logits.append(self.forward(x))\n",
    "        return torch.cat(logits, axis=0)\n",
    "    \n",
    "net = RNNModel(2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=.001)\n",
    "# Get data in proper format\n",
    "dic, target = np.unique(meta[:, :1], return_inverse=True)\n",
    "target = torch.Tensor(target.reshape((-1, 1)))\n",
    "assert target.max() < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0bdd79-dac7-41ee-8e8a-75353cf3b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  \tLoss: 0.68914\n",
      "Epoch: 1  \tLoss: 0.66122\n",
      "Epoch: 2  \tLoss: 0.71097\n",
      "Epoch: 3  \tLoss: 0.72839\n",
      "Epoch: 4  \tLoss: 0.68309\n",
      "Epoch: 5  \tLoss: 0.68761\n",
      "Epoch: 6  \tLoss: 0.66803\n",
      "Epoch: 7  \tLoss: 0.68593\n",
      "Epoch: 8  \tLoss: 0.69525\n",
      "Epoch: 9  \tLoss: 0.69784\n",
      "Epoch: 10  \tLoss: 0.63768\n",
      "Epoch: 11  \tLoss: 0.68339\n",
      "Epoch: 12  \tLoss: 0.64806\n",
      "Epoch: 13  \tLoss: 0.73733\n",
      "Epoch: 14  \tLoss: 0.67241\n",
      "Epoch: 15  \tLoss: 0.74796\n",
      "Epoch: 16  \tLoss: 0.71808\n",
      "Epoch: 17  \tLoss: 0.67623\n",
      "Epoch: 18  \tLoss: 0.67157\n",
      "Epoch: 19  \tLoss: 0.69826\n",
      "Epoch: 20  \tLoss: 0.69544\n",
      "Epoch: 21  \tLoss: 0.66830\n",
      "Epoch: 22  \tLoss: 0.65206\n",
      "Epoch: 23  \tLoss: 0.68289\n",
      "Epoch: 24  \tLoss: 0.68872\n",
      "Epoch: 25  \tLoss: 0.72199\n",
      "Epoch: 26  \tLoss: 0.67543\n",
      "Epoch: 27  \tLoss: 0.68391\n",
      "Epoch: 28  \tLoss: 0.68833\n",
      "Epoch: 29  \tLoss: 0.66101\n",
      "Epoch: 30  \tLoss: 0.72865\n",
      "Epoch: 31  \tLoss: 0.64320\n",
      "Epoch: 32  \tLoss: 0.65002\n",
      "Epoch: 33  \tLoss: 0.68394\n",
      "Epoch: 34  \tLoss: 0.75467\n",
      "Epoch: 35  \tLoss: 0.69214\n",
      "Epoch: 36  \tLoss: 0.66545\n",
      "Epoch: 37  \tLoss: 0.67240\n",
      "Epoch: 38  \tLoss: 0.68564\n",
      "Epoch: 39  \tLoss: 0.71779\n",
      "Epoch: 40  \tLoss: 0.67889\n",
      "Epoch: 41  \tLoss: 0.72450\n",
      "Epoch: 42  \tLoss: 0.70720\n",
      "Epoch: 43  \tLoss: 0.68328\n",
      "Epoch: 44  \tLoss: 0.70210\n",
      "Epoch: 45  \tLoss: 0.70893\n",
      "Epoch: 46  \tLoss: 0.66902\n",
      "Epoch: 47  \tLoss: 0.66277\n",
      "Epoch: 48  \tLoss: 0.64719\n",
      "Epoch: 49  \tLoss: 0.68423\n",
      "Epoch: 50  \tLoss: 0.68807\n",
      "Epoch: 51  \tLoss: 0.69105\n",
      "Epoch: 52  \tLoss: 0.68260\n",
      "Epoch: 53  \tLoss: 0.69239\n",
      "Epoch: 54  \tLoss: 0.71608\n",
      "Epoch: 55  \tLoss: 0.70713\n",
      "Epoch: 56  \tLoss: 0.69262\n",
      "Epoch: 57  \tLoss: 0.67473\n",
      "Epoch: 58  \tLoss: 0.69181\n",
      "Epoch: 59  \tLoss: 0.66922\n",
      "Epoch: 60  \tLoss: 0.72866\n",
      "Epoch: 61  \tLoss: 0.68220\n",
      "Epoch: 62  \tLoss: 0.71711\n",
      "Epoch: 63  \tLoss: 0.68857\n",
      "Epoch: 64  \tLoss: 0.62317\n",
      "Epoch: 65  \tLoss: 0.71233\n",
      "Epoch: 66  \tLoss: 0.72110\n",
      "Epoch: 67  \tLoss: 0.65312\n",
      "Epoch: 68  \tLoss: 0.69647\n",
      "Epoch: 69  \tLoss: 0.67624\n",
      "Epoch: 70  \tLoss: 0.70055\n",
      "Epoch: 71  \tLoss: 0.69819\n",
      "Epoch: 72  \tLoss: 0.71173\n",
      "Epoch: 73  \tLoss: 0.65903\n",
      "Epoch: 74  \tLoss: 0.68091\n",
      "Epoch: 75  \tLoss: 0.68132\n",
      "Epoch: 76  \tLoss: 0.72136\n",
      "Epoch: 77  \tLoss: 0.69284\n",
      "Epoch: 78  \tLoss: 0.67065\n",
      "Epoch: 79  \tLoss: 0.69433\n",
      "Epoch: 80  \tLoss: 0.69714\n",
      "Epoch: 81  \tLoss: 0.70974\n",
      "Epoch: 82  \tLoss: 0.69640\n",
      "Epoch: 83  \tLoss: 0.65961\n",
      "Epoch: 84  \tLoss: 0.67161\n",
      "Epoch: 85  \tLoss: 0.68515\n",
      "Epoch: 86  \tLoss: 0.66931\n",
      "Epoch: 87  \tLoss: 0.66305\n",
      "Epoch: 88  \tLoss: 0.64653\n",
      "Epoch: 89  \tLoss: 0.66520\n",
      "Epoch: 90  \tLoss: 0.68737\n",
      "Epoch: 91  \tLoss: 0.70488\n",
      "Epoch: 92  \tLoss: 0.70661\n",
      "Epoch: 93  \tLoss: 0.67935\n",
      "Epoch: 94  \tLoss: 0.71560\n",
      "Epoch: 95  \tLoss: 0.67599\n",
      "Epoch: 96  \tLoss: 0.67612\n",
      "Epoch: 97  \tLoss: 0.69645\n",
      "Epoch: 98  \tLoss: 0.68187\n",
      "Epoch: 99  \tLoss: 0.66283\n",
      "Epoch: 100  \tLoss: 0.69372\n",
      "Epoch: 101  \tLoss: 0.66853\n",
      "Epoch: 102  \tLoss: 0.69190\n",
      "Epoch: 103  \tLoss: 0.68969\n",
      "Epoch: 104  \tLoss: 0.64796\n",
      "Epoch: 105  \tLoss: 0.67166\n",
      "Epoch: 106  \tLoss: 0.66930\n",
      "Epoch: 107  \tLoss: 0.69536\n",
      "Epoch: 108  \tLoss: 0.62404\n",
      "Epoch: 109  \tLoss: 0.71036\n",
      "Epoch: 110  \tLoss: 0.67770\n",
      "Epoch: 111  \tLoss: 0.68715\n",
      "Epoch: 112  \tLoss: 0.67916\n",
      "Epoch: 113  \tLoss: 0.67097\n",
      "Epoch: 114  \tLoss: 0.64691\n",
      "Epoch: 115  \tLoss: 0.65924\n",
      "Epoch: 116  \tLoss: 0.70877\n",
      "Epoch: 117  \tLoss: 0.70576\n",
      "Epoch: 118  \tLoss: 0.69238\n",
      "Epoch: 119  \tLoss: 0.66273\n",
      "Epoch: 120  \tLoss: 0.70482\n",
      "Epoch: 121  \tLoss: 0.70073\n",
      "Epoch: 122  \tLoss: 0.70403\n",
      "Epoch: 123  \tLoss: 0.70115\n",
      "Epoch: 124  \tLoss: 0.68878\n",
      "Epoch: 125  \tLoss: 0.68231\n",
      "Epoch: 126  \tLoss: 0.72129\n",
      "Epoch: 127  \tLoss: 0.64751\n",
      "Epoch: 128  \tLoss: 0.69730\n",
      "Epoch: 129  \tLoss: 0.68895\n",
      "Epoch: 130  \tLoss: 0.67903\n",
      "Epoch: 131  \tLoss: 0.69625\n",
      "Epoch: 132  \tLoss: 0.68175\n",
      "Epoch: 133  \tLoss: 0.66386\n",
      "Epoch: 134  \tLoss: 0.70826\n",
      "Epoch: 135  \tLoss: 0.67622\n",
      "Epoch: 136  \tLoss: 0.69174\n",
      "Epoch: 137  \tLoss: 0.68248\n",
      "Epoch: 138  \tLoss: 0.66988\n",
      "Epoch: 139  \tLoss: 0.69377\n",
      "Epoch: 140  \tLoss: 0.67053\n",
      "Epoch: 141  \tLoss: 0.67146\n",
      "Epoch: 142  \tLoss: 0.66213\n",
      "Epoch: 143  \tLoss: 0.69453\n",
      "Epoch: 144  \tLoss: 0.64735\n",
      "Epoch: 145  \tLoss: 0.66844\n",
      "Epoch: 146  \tLoss: 0.68275\n",
      "Epoch: 147  \tLoss: 0.71303\n",
      "Epoch: 148  \tLoss: 0.71146\n",
      "Epoch: 149  \tLoss: 0.67439\n",
      "Epoch: 150  \tLoss: 0.69701\n",
      "Epoch: 151  \tLoss: 0.67198\n",
      "Epoch: 152  \tLoss: 0.69258\n",
      "Epoch: 153  \tLoss: 0.65623\n",
      "Epoch: 154  \tLoss: 0.66067\n",
      "Epoch: 155  \tLoss: 0.70736\n",
      "Epoch: 156  \tLoss: 0.71026\n",
      "Epoch: 157  \tLoss: 0.67886\n",
      "Epoch: 158  \tLoss: 0.67243\n",
      "Epoch: 159  \tLoss: 0.65827\n",
      "Epoch: 160  \tLoss: 0.69987\n",
      "Epoch: 161  \tLoss: 0.70290\n",
      "Epoch: 162  \tLoss: 0.65800\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "subsample = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(len(exp)//batch_size):\n",
    "        # Get batch\n",
    "        batch_idx = np.random.choice(len(exp), batch_size)\n",
    "        stim, resp = [], []\n",
    "        for i in batch_idx:\n",
    "            stim.append(torch.Tensor(exp[i]['stim']))\n",
    "            resp.append(torch.Tensor(exp[i]['resp']))\n",
    "        inp = [torch.stack([st, re], axis=1)[::subsample] for st, re in zip(stim, resp)]\n",
    "        \n",
    "        # Get outputs\n",
    "        logits = net.run_all(inp)\n",
    "        true = target[batch_idx]\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits, true)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # CLI\n",
    "    print(f'Epoch: {epoch}  \\tLoss: {float(loss.detach()):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc77f9f-d82f-4bc2-818c-09de0d2e6ee2",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6979bf02-14d7-46cb-89df-9167bb594561",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim, resp = [], []\n",
    "for i in range(len(exp)):\n",
    "    stim.append(torch.Tensor(exp[i]['stim']))\n",
    "    resp.append(torch.Tensor(exp[i]['resp']))\n",
    "inp = [torch.stack([st, re], axis=1)[::subsample] for st, re in zip(stim, resp)]\n",
    "logits = net.run_all(inp)\n",
    "logits = (logits > .5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa812b3-3c89-44aa-a039-0da77b00f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(dic[logits], return_counts=True)[1]\n",
    "# np.argwhere(dic[logits] == 'aspiny')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95ae45-95dd-44f7-b820-cb1759448253",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ff6b4-e631-431d-b9bc-475e45d03ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "sample = 25\n",
    "axes[0].plot(exp[sample]['time'][::subsample], exp[sample]['resp'][::subsample], color='black')\n",
    "axes[1].plot(exp[sample]['time'][::subsample], exp[sample]['stim'][::subsample], color='gray')\n",
    "axes[0].set_ylabel(\"mV\")\n",
    "axes[1].set_ylabel(\"pA\")\n",
    "axes[1].set_xlabel(\"seconds\")\n",
    "plt.show()\n",
    "print(f'True:\\t{dic[(target > .5) * 1][sample]}\\nPred:\\t{dic[logits.detach()][sample]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661e10a-f54c-44e8-97c4-087232ca1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot processed\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "sample = 25\n",
    "_, out = net(torch.stack([torch.Tensor(exp[sample]['resp']), torch.Tensor(exp[sample]['stim'])], axis=1)[::subsample], return_out=True)\n",
    "axes[0].plot(exp[sample]['time'][::subsample], out[:, 0].detach(), color='black')\n",
    "axes[1].plot(exp[sample]['time'][::subsample], out[:, 2].detach(), color='black')\n",
    "axes[0].set_ylabel(None)\n",
    "axes[1].set_ylabel(None)\n",
    "axes[1].set_xlabel(\"seconds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec7ebc-e429-4edf-a0e8-baea23c19641",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef0234-4080-4a8b-ba06-ec51cc783b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "sample = 39\n",
    "axes[0].plot(exp[sample]['time'], exp[sample]['resp'], color='black')\n",
    "axes[1].plot(exp[sample]['time'], exp[sample]['stim'], color='gray')\n",
    "axes[0].set_ylabel(\"mV\")\n",
    "axes[1].set_ylabel(\"pA\")\n",
    "axes[1].set_xlabel(\"seconds\")\n",
    "plt.show()\n",
    "print(f'True:\\t{dic[(target > .5) * 1][sample]}\\nPred:\\t{dic[logits][sample]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13de296-912a-4f30-899e-76f219f016b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot processed\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "sample = 39\n",
    "_, out = net(torch.stack([torch.Tensor(exp[sample]['resp']), torch.Tensor(exp[sample]['stim'])], axis=1)[::subsample], return_out=True)\n",
    "axes[0].plot(exp[sample]['time'][::subsample], out[:, 10].detach(), color='black')\n",
    "axes[1].plot(exp[sample]['time'][::subsample], out[:, 12].detach(), color='black')\n",
    "axes[0].set_ylabel(None)\n",
    "axes[1].set_ylabel(None)\n",
    "axes[1].set_xlabel(\"seconds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867882a0-c352-4033-985f-136e32ca47a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
